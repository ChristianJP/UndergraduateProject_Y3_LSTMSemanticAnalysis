{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "5a864243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christianpollitt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christianpollitt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/christianpollitt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "nltk.download('wordnet')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "420d0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    def __init__(self):\n",
    "        self.path = \"./product_reviews/\"\n",
    "         \n",
    "    def read_data_raw(self, path: str) -> list:         \n",
    "        \"\"\"\n",
    "        Read files from a directory and then append the data of each file into a list.\n",
    "        \"\"\"\n",
    "        corpus_root = path\n",
    "        corpora = PlaintextCorpusReader(corpus_root, '.*')\n",
    "        \n",
    "        return corpora\n",
    "    \n",
    "    def process_review(self, review, stem=False, lem=False) -> list:\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(review)\n",
    "        # Case fold and Remove Stop Words\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        processed_list = [w for w in tokens if w.lower() not in stopwords and w.isalpha() and (len(w) > 1) and  w.lower()]\n",
    "        \n",
    "        if (stem):\n",
    "            stemmer = PorterStemmer()\n",
    "            processed_list = [stemmer.stem(x) for x in processed_list]\n",
    "        \n",
    "        if (lem):\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            # lemmatization\n",
    "            processed_list = [lemmatizer.lemmatize(x) for x in processed_list]\n",
    "        \n",
    "\n",
    "        return processed_list\n",
    "    \n",
    "    def extract_label(self, review: str) -> str:  \n",
    "        score = 0\n",
    "        # print(review)\n",
    "        ratings = re.findall(r'\\[.[0-9]\\]', review)\n",
    "        \n",
    "        # If no rating given, set to NA\n",
    "        label = \"NA\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Collate a total score for reviews, apply a class label\n",
    "        for rate in ratings:\n",
    "            rate = str(rate).replace('[','')\n",
    "            rate = str(rate).replace(']','')\n",
    "            pos_sign = '+'\n",
    "            neg_sign = '-'\n",
    "            if (pos_sign in rate):\n",
    "                rate = rate.replace('+',\"\")\n",
    "                rate = int(rate)\n",
    "            elif (neg_sign in rate):\n",
    "                rate = rate.replace('-',\"\")\n",
    "                rate = int(rate)\n",
    "                rate = -1*rate\n",
    "                \n",
    "            score += int(rate)\n",
    "        \n",
    "        if (score > 0):\n",
    "            label = 1\n",
    "        elif (score < 0):\n",
    "            label = 0\n",
    "        elif (score == 0):\n",
    "            label = -1\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def prepare_reviews_and_labels(self, review_files):\n",
    "        # For each file in the folder\n",
    "        review_bank = []\n",
    "        processed_review_bank = []\n",
    "        review_labels = []\n",
    "        for file in review_files:\n",
    "            with open((\"product_reviews/\"+file), \"r+\", encoding='utf-8') as f:\n",
    "                \n",
    "                # Put all the file data into separate lines\n",
    "                lines = f.readlines()\n",
    "                n_lines = len(lines)\n",
    "                n_reviews = 0\n",
    "                review_data = []\n",
    "        \n",
    "                # For each line in the file, check if it contains a start of the review tag\n",
    "                for i in range(n_lines):\n",
    "                    if \"[t]\" in lines[i]:\n",
    "                        # If there is sentences from the last tag\n",
    "                        if (len(review_data) != 0):\n",
    "                            review_bank.append(review_data)\n",
    "                            # Restart the compilation of sentences\n",
    "                        review_data = []    \n",
    "                        n_reviews += 1\n",
    "                        # Create a place to store the upcoming sentences\n",
    "                    else:\n",
    "                        # Get the sentence and append it to to the last documented review\n",
    "                        sentence = lines[i].split(\"##\")\n",
    "                        review_data.append(sentence)\n",
    "                        \n",
    "                # For each Review, Process and Save review\n",
    "                for review in review_data:\n",
    "                    # Process Each Review, get the label (by processing it as a string for scores)\n",
    "                    review_string = ''.join(review)\n",
    "                    label = self.extract_label(review_string)\n",
    "                    \n",
    "                    # Get a list of terms in the review\n",
    "                    processed_review = self.process_review(review_string, True, True)\n",
    "                    processed_review_bank.append(processed_review)\n",
    "                    review_labels.append(label)\n",
    "\n",
    "        return processed_review_bank, review_labels\n",
    "    \n",
    "    def prepare_train_test_data(self, features_positive, features_negative):\n",
    "        \"\"\"\n",
    "        Split data into train validation and testing\n",
    "        \"\"\"\n",
    "        ratio_traintest = 0.8\n",
    "        # Get the data for train and test\n",
    "        df_positive_train, df_positive_test = train_test_split(features_positive, train_size = ratio_traintest, random_state = 1)\n",
    "        df_negative_train, df_negative_test = train_test_split(features_negative, train_size = ratio_traintest, random_state = 1)\n",
    "        \n",
    "        df_train = pd.concat([df_positive_train, df_negative_train], ignore_index=True, sort=False)\n",
    "        df_test = pd.concat([df_positive_test, df_negative_test], ignore_index=True, sort=False)\n",
    "        \n",
    "        # Split the test into validation / test data\n",
    "        df_test, df_validation =  train_test_split(df_test, train_size = 0.5, random_state = 1)\n",
    "\n",
    "        return df_train, df_test, df_validation\n",
    "    \n",
    "    \n",
    "    def prepare_train_test_data_kfold(self, features_test):\n",
    "        \"\"\"\n",
    "        Split data into train validation and testing\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Split the test into validation / test data\n",
    "        df_test, df_validation =  train_test_split(features_test, train_size = 0.5)\n",
    "        \n",
    "        return df_test, df_validation\n",
    "    \n",
    "    def prepare_for_pytorch(self, train_x, train_y, validation_x, validation_y, test_x, test_y):\n",
    "        \"\"\"\n",
    "        Split data into train validation and testing\n",
    "        \"\"\"\n",
    "        batch_size = 16\n",
    "        \n",
    "        # Format data to correct dimensions for Data Loader\n",
    "        train_x = np.vstack(train_x).astype(np.int64)\n",
    "        train_y = np.vstack(train_y).astype(np.int64)\n",
    "        validation_x = np.vstack(validation_x).astype(np.int64)\n",
    "        validation_y = np.vstack(validation_y).astype(np.int64)\n",
    "        test_x = np.vstack(test_x).astype(np.int64)\n",
    "        test_y = np.vstack(test_y).astype(np.int64)\n",
    "        \n",
    "        train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "        validation_data = TensorDataset(torch.from_numpy(validation_x), torch.from_numpy(validation_y))\n",
    "        test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "        train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "        validation_loader = DataLoader(validation_data, shuffle=True, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "        \n",
    "        return train_loader, validation_loader, test_loader\n",
    "\n",
    "    \n",
    "    def prepare_review_vocab(self, review_bank):\n",
    "        \"\"\"\n",
    "        Word embedding, by creating a numbered dictionary to later convert to one-hot vectors\n",
    "        \"\"\"\n",
    "        all_words = []\n",
    "        \n",
    "        # Combine all the data to make a Vocabulary\n",
    "        for review in review_bank:\n",
    "            review_words = []\n",
    "            for i, word in enumerate(review):\n",
    "                review_words.append(review[i])\n",
    "            all_words.extend(review_words)\n",
    "            \n",
    "        # Create vocab\n",
    "        vocab = sorted(set(all_words))\n",
    "        vocab_size = len(vocab)\n",
    "             \n",
    "        # Create Integers for each word\n",
    "        vocab_int_dict = {word: i+1 for i, word in enumerate(vocab)}\n",
    "        \n",
    "        # Convert Reviews using One-Hot representation\n",
    "        reviews_oneh_indexs = []\n",
    "        for review in review_bank:\n",
    "            review_index_form = []\n",
    "            for word in review:\n",
    "                # Find the index in the dictionary \n",
    "                index = vocab_int_dict[word]\n",
    "                review_index_form.append(index)\n",
    "                \n",
    "            reviews_oneh_indexs.append(review_index_form)   \n",
    "            \n",
    "        return vocab_int_dict, reviews_oneh_indexs\n",
    "    \n",
    "    def prepare_padded_reviews(self, reviews_as_index):\n",
    "        \"\"\"\n",
    "        Padding so that reviews of varied length can be processed\n",
    "        \"\"\"\n",
    "        # print(reviews_as_index)\n",
    "        # Fine the largest Review\n",
    "        max_length = 0\n",
    "        for single_review_int in reviews_as_index:\n",
    "        # Establish the max length\n",
    "            if (len(single_review_int) > max_length):\n",
    "                max_length = len(single_review_int)\n",
    "                \n",
    "        # Convert to One-Hot form\n",
    "        one_hot_vec = np.zeros((len(reviews_as_index), max_length), dtype=int)\n",
    "                \n",
    "        for i, review in enumerate(reviews_as_index):\n",
    "            review_length = len(review)\n",
    "            # If we need to do any padding at all\n",
    "            if review_length <= max_length:\n",
    "                extension_length = max_length-review_length\n",
    "                padding = list(np.zeros(extension_length))\n",
    "                padded_review = padding+review\n",
    "        \n",
    "            one_hot_vec[i, :] = np.array(padded_review)\n",
    "\n",
    "        return one_hot_vec, max_length\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "f765838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        # dropout layer, a hyperparameter which I shall modify lots\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "      \n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "3b2a6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_lstm(lstm, train_loader, evaluation_loader, max_length):\n",
    "    \n",
    "    lr = 0.001\n",
    "    epochs = 7\n",
    "    clip = 5\n",
    "    print_every = 200\n",
    "    batch_size = 16\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.AdamW(lstm.parameters(), lr=lr)\n",
    "    CUDA_VISIBLE_DEVICES=\"\"\n",
    "    counter = 0\n",
    "    lstm.train()\n",
    "\n",
    "    # Train for the allocated epochs\n",
    "    for e in range(epochs):\n",
    "        # Initialize hidden states\n",
    "        h = lstm.init_hidden(batch_size)\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            if((inputs.shape[0], inputs.shape[1]) != (batch_size, max_length)):\n",
    "                #print('Training - Input Shape Issue:', inputs.shape)\n",
    "                continue\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            # Create the hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            lstm.zero_grad()\n",
    "\n",
    "             # Make predictions // get the output from the model\n",
    "            inputs = inputs.type(torch.LongTensor)\n",
    "            output, h = lstm(inputs, h)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output.unsqueeze(1), labels.float())\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(lstm.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute loss statistics\n",
    "            if counter % print_every == 0:\n",
    "\n",
    "                # Get validation loss\n",
    "                val_h = lstm.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                lstm.eval()\n",
    "                for inputs, labels in evaluation_loader:\n",
    "                    if((inputs.shape[0], inputs.shape[1]) != (batch_size, max_length)):\n",
    "                        # print('Validation - Input Shape Issue:', inputs.shape)\n",
    "                        continue\n",
    "\n",
    "                    # Reinitialize hidden states\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    inputs = inputs.type(torch.LongTensor)\n",
    "                    output, val_h = lstm(inputs, val_h)\n",
    "                    val_loss = criterion(output.unsqueeze(1), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                lstm.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "\n",
    "    return lstm, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "4cd5dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lstm_model(lstm, criterion, test_loader, max_length):\n",
    "    \"\"\"\n",
    "    Test the performance of the model, using test data\n",
    "    \"\"\"\n",
    "    test_losses = []\n",
    "    num_correct = 0\n",
    "    batch_size = 16\n",
    "    \n",
    "    # Create the hidden states\n",
    "    h = lstm.init_hidden(batch_size)\n",
    "\n",
    "    lstm.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        if((inputs.shape[0], inputs.shape[1]) != (batch_size, max_length)):\n",
    "            # print('Testing - Input Shape Issue:', inputs.shape)\n",
    "            continue\n",
    "\n",
    "        # Reinitialize hidden layer\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # Make predictions // get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        output, h = lstm(inputs, h)\n",
    "\n",
    "         # Calculate the loss and then perform backpropagation\n",
    "        test_loss = criterion(output.unsqueeze(1), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        # convert output probabilities to Positive-1 or Negative-0 Review label\n",
    "        pred = torch.round(output.squeeze())\n",
    "\n",
    "        # Checking predictions againts true review labels\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "\n",
    "    print(\"Test loss: {:.2f}\".format(np.mean(test_losses)))\n",
    "    test_acc = num_correct/len(test_loader.dataset)\n",
    "    print(\"Test accuracy: {:.2f}\".format(test_acc))\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "6654d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing LSTM Analysis using 80/20 Train/Test split\n",
      "Test loss: 0.77\n",
      "Test accuracy: 0.52\n",
      "Random Selection of 80/20 Train/Test split |  Test Accuracy: 0.5238095238095238\n",
      "\n",
      "\n",
      "Performing LSTM Analysis using 5 Fold CV\n",
      "Performing Fold\n",
      "Test loss: 0.86\n",
      "Test accuracy: 0.67\n",
      "Performing Fold\n",
      "Test loss: 3.25\n",
      "Test accuracy: 0.45\n",
      "Performing Fold\n",
      "Test loss: 1.50\n",
      "Test accuracy: 0.60\n",
      "Performing Fold\n",
      "Test loss: 0.03\n",
      "Test accuracy: 0.80\n",
      "Performing Fold\n",
      "Test loss: 0.49\n",
      "Test accuracy: 0.65\n",
      "5 Fold Cross Validation | (Mean) Test Accuracy: 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "lstm_net = None\n",
    "\n",
    "def main():\n",
    "    def iteration() -> int:\n",
    "        \"\"\"Main Call / Demonstration -----------------------------------------\"\"\"\n",
    "        \n",
    "        index = Functions() # initilaise the index\n",
    "        \n",
    "        # 1. Data Extraction\n",
    "        # Get the working Directory and change to it\n",
    "        cwd = os.getcwd()\n",
    "        os.chdir(cwd)\n",
    "        \n",
    "        # Get the list of Reviews\n",
    "        all_review_files = os.listdir(\"./product_reviews/\")\n",
    "        # Remove Noisey file (incorrectly labelled and also the README.txt)\n",
    "        all_review_files.remove('README.txt')\n",
    "        all_review_files.remove('Canon_PowerShot_SD500.txt')\n",
    "        review_bank, review_labels = index.prepare_reviews_and_labels(all_review_files)\n",
    "        \n",
    "        # Get the Reviews as Word Embeddings in Int form\n",
    "        vocab_dict, reviews_as_index = index.prepare_review_vocab(review_bank)\n",
    "        \n",
    "        # Pad the Reviews so that they are all the same length and get in One-Hot form \n",
    "        one_hot_features, max_length = index.prepare_padded_reviews(reviews_as_index)\n",
    "            \n",
    "        # 2. Data Preprocessing\n",
    "        # Get the reviews using indexs of positive/negative\n",
    "        neutral_review_indexs = [i for i,x in enumerate(review_labels) if x==-1]\n",
    "        # Separate Positive / Negative Reviews\n",
    "        pos_review_indexs = [i for i,x in enumerate(review_labels) if x==1]\n",
    "        neg_review_indexs = [i for i,x in enumerate(review_labels) if x==0]\n",
    "        \n",
    "        # Get the One-hot features for given indexs\n",
    "        list_positive = [one_hot_features[index] for index in pos_review_indexs]\n",
    "        list_negative = [one_hot_features[index] for index in neg_review_indexs]\n",
    "\n",
    "        # Make dataframes for positive data -> 'review' | 'y'\n",
    "        pos_labels = []\n",
    "        for i in range(len(list_positive)):\n",
    "            pos_labels.append(1)\n",
    "            \n",
    "        neg_labels = []\n",
    "        for i in range(len(list_negative)):\n",
    "            neg_labels.append(0)\n",
    "            \n",
    "        df_positive  = pd.DataFrame(list(zip(list_positive, pos_labels)), columns=['review', 'y'])\n",
    "        df_negative= pd.DataFrame(list(zip(list_negative, neg_labels)), columns=['review', 'y'])\n",
    "        df_all  = df_positive.append(df_negative)\n",
    "        \n",
    "        # Make dataframes for Negative data -> 'review' | 'y'\n",
    "        \n",
    "        print(\"Performing LSTM Analysis using 80/20 Train/Test split\")\n",
    "\n",
    "        # Separate trainin/test data\n",
    "        training_data, testing_data, validation_data = index.prepare_train_test_data(df_positive, df_negative)\n",
    "        \n",
    "        # 3. Prepare for LSTM Single Run\n",
    "        train_x = training_data['review'].to_numpy()\n",
    "        train_y = training_data['y'].to_numpy()\n",
    "        validation_x = validation_data['review'].to_numpy()\n",
    "        validation_y = validation_data['y'].to_numpy()\n",
    "        test_x = testing_data['review'].to_numpy()\n",
    "        test_y = testing_data['y'].to_numpy()\n",
    "        \n",
    "        train_loader, valid_loader, test_loader  = index.prepare_for_pytorch(train_x, train_y, validation_x, validation_y, test_x, test_y)  \n",
    "        \n",
    "        lstm = SentimentLSTM(len(vocab_dict), 1, 100, 512, 3)\n",
    "        lstm, criterion = train_evaluate_lstm(lstm, train_loader, valid_loader, max_length)\n",
    "        acc_single = test_lstm_model(lstm, criterion, test_loader, max_length)\n",
    "        \n",
    "        print(\"Random Selection of 80/20 Train/Test split |  Test Accuracy: \" + str(acc_single))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "        \n",
    "        print(\"Performing LSTM Analysis using 5 Fold CV\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"5 Fold CV -----------------------------------------\"\"\"\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "        \n",
    "        X = df_all['review'].to_numpy()\n",
    "        y = df_all['y'].to_numpy()\n",
    "        \n",
    "        cv_accs = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(df_all):\n",
    "            # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            print(\"Performing Fold\")\n",
    "            \n",
    "            # Separate the train/test data\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            \n",
    "            # print(X_test)\n",
    "            \n",
    "            test_dataset = pd.DataFrame()\n",
    "            test_dataset['review'] = X_test\n",
    "            test_dataset['y'] = y_test\n",
    "\n",
    "            # Create training data/test/validation\n",
    "            testing_data, validation_data = index.prepare_train_test_data_kfold(test_dataset)\n",
    "            \n",
    "            # print(testing_data)\n",
    "            train_x = X_train\n",
    "            train_y = y_train\n",
    "            validation_x = validation_data['review'].to_numpy()\n",
    "            validation_y = validation_data['y'].to_numpy()\n",
    "            test_x = testing_data['review'].to_numpy()\n",
    "            test_y = testing_data['y'].to_numpy()\n",
    "\n",
    "            train_loader, valid_loader, test_loader  = index.prepare_for_pytorch(train_x, train_y, validation_x, validation_y, test_x, test_y) \n",
    "            \n",
    "            lstm = SentimentLSTM(len(vocab_dict), 1, 1000, 512, 3)\n",
    "    \n",
    "            lstm, criterion = train_evaluate_lstm(lstm, train_loader, valid_loader, max_length)\n",
    "            acc = test_lstm_model(lstm, criterion, test_loader, max_length) \n",
    "            \n",
    "            cv_accs.append(acc)\n",
    "        \n",
    "        mean_acc = np.mean(cv_accs)\n",
    "        print(\"5 Fold Cross Validation | (Mean) Test Accuracy: \" + str(mean_acc))\n",
    "    lstm_net = iteration()\n",
    "\n",
    "    \n",
    "    \n",
    "extraction = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
