{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DemoNotebook-LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook to Manually Test LSTM Model**"
      ],
      "metadata": {
        "id": "e7eNromhz_62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "6KBDxhh70X7B"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOxyWO3q0a1V",
        "outputId": "a03578fa-c78d-4c63-83d4-b365587deca3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU is available\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"GPU not available, CPU used\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk24Yrwk1Zk_",
        "outputId": "f07234c6-6633-4d87-88ed-156c7fc598f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not available, CPU used\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model class"
      ],
      "metadata": {
        "id": "XM1CLrUX2Esk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim=64, \n",
        "               output_dim=1, drop_prob=0.5, bidirectional=True):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.no_layers = no_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    # embedding and LSTM layers\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    #lstm\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                        hidden_size=self.hidden_dim,\n",
        "                        num_layers=no_layers,\n",
        "                        batch_first=True,\n",
        "                        bidirectional=bidirectional)\n",
        "    \n",
        "    # dropout layer\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    # linear and sigmoid layer\n",
        "    self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # embeddings and lstm_out\n",
        "    embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
        "    \n",
        "    # dropout and fully connected layer\n",
        "    out = self.dropout(lstm_out)\n",
        "    out = self.fc(out)\n",
        "    sig_out = self.sig(out)\n",
        "    \n",
        "    # reshape to be batch_size first\n",
        "    sig_out = sig_out.view(batch_size, -1)\n",
        "    sig_out = sig_out[:, -1] # get last batch of labels\n",
        "    \n",
        "    # return last sigmoid output and hidden state\n",
        "    return sig_out, hidden\n",
        "\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    ''' Initializes hidden state '''\n",
        "\n",
        "    # Create two new tensors with sizes (n_layers x 2) x batch_size x hidden_dim\n",
        "    # initialized to zero, for hidden state and cell state of LSTM\n",
        "    # n_layers x 2 because it is bidirectional LSTM\n",
        "    h0 = torch.zeros((self.no_layers*2, batch_size, self.hidden_dim)).to(device)\n",
        "    c0 = torch.zeros((self.no_layers*2, batch_size, self.hidden_dim)).to(device)\n",
        "    hidden = (h0, c0)\n",
        "\n",
        "    return hidden"
      ],
      "metadata": {
        "id": "IL691lbC1yak"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions to pre-process the raw text"
      ],
      "metadata": {
        "id": "lomxqHCa2HPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H7MUJUxYz-TK"
      },
      "outputs": [],
      "source": [
        "def preprocess_string(s):\n",
        "  '''\n",
        "  Helper function to process a message\n",
        "  '''\n",
        "  # Remove all non-word characters (everything except numbers and letters)\n",
        "  s = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "\n",
        "  # Replace all runs of whitespaces with no space\n",
        "  s = re.sub(r\"\\s+\", '', s)\n",
        "  \n",
        "  # replace digits with no space\n",
        "  s = re.sub(r\"\\d\", '', s)\n",
        "\n",
        "  return s\n",
        "\n",
        "def padding_(sentences, seq_len):\n",
        "  features = np.zeros((len(sentences), seq_len), dtype=int)\n",
        "  for ii, review in enumerate(sentences):\n",
        "    if len(review) != 0:\n",
        "      features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "  return features\n",
        "\n",
        "def predict(text, vocab, path='./drive/MyDrive/Colab Notebooks/Models/state_dict.pt'):\n",
        "  word_seq = np.array([vocab[preprocess_string(word)] for word in text.lower().split() \n",
        "                         if preprocess_string(word) in vocab.keys()])\n",
        "  word_seq = np.expand_dims(word_seq, axis=0)\n",
        "  pad = torch.from_numpy(padding_(word_seq, 30))\n",
        "  inputs = pad.to(device)\n",
        "  batch_size = 1\n",
        "\n",
        "  model = torch.load(path)\n",
        "  model.eval()\n",
        "\n",
        "  h = model.init_hidden(batch_size)\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  output, h = model(inputs, h)\n",
        "  return(output.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model and the dictionary and get the input from the user."
      ],
      "metadata": {
        "id": "_OzX9VyX2Nwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_file = open(\"./drive/MyDrive/Colab Notebooks/data.pkl\", \"rb\")\n",
        "vocab = pickle.load(a_file)\n",
        "\n",
        "message = input(\"Enter message: \")\n",
        "p = predict(message, vocab)\n",
        "status = \"bullish\" if p > 0.5 else \"bearish\"\n",
        "p = (1 - p) if status == \"bearish\" else p\n",
        "print(f'Predicted status is {status} with a probability of {p}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuFryj4r0iLG",
        "outputId": "7e19b1ef-1446-4102-c261-a07e0c4f972a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter message: Volume looking good\n",
            "Predicted status is bullish with a probability of 0.994971752166748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problematic cases**\n",
        "\n",
        "\"How can you be happy woth this price\" - identified as bullish with high confidence but it is actually bearish, as the price is really low.\n",
        "\n",
        "\"It's not gonna rise\" - negations not recognised due to stop-word removal\n",
        "\n",
        "\"Price not good\" - again, classified as bullish due to lack of such cases in the training data.\n",
        "\n",
        "\"If it reaches 50, I'd be delighted\" - bullish with low confidence. It implies bullish but the model classified as bullish almost by chance (51% bullish vs 49% bearish)\n",
        "\n",
        "\"I wish it was going up instead\" - clearly bearish since the author wishes the opposite was happening to what is actually happening (dropping)."
      ],
      "metadata": {
        "id": "5nOiJYnx30kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uKokt23y0myy"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}